{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial audio study and the necessary text extraction, the next logical step will be the stduy of this type of unstructured data.\n",
    "\n",
    "We will take a look at both the audio transcripts and the captions from the videos, in order to extract as much information as possible to predict the virality of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First libraries and variables import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the folder containing the module\n",
    "root_dir = Path.cwd().resolve().parent.parent\n",
    "\n",
    "# Add the folder path to sys.path\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils import load_json, load_transcriptions, clean_text, plot_freq_dist, analyze_text\n",
    "from config.variables import text_path, json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folder = os.path.join(root_dir, text_path)\n",
    "json_path = os.path.join(root_dir, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data recopilation and ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transcriptions of the videos\n",
    "transcriptions = load_transcriptions(text_folder)\n",
    "\n",
    "# Load the information from the JSON file\n",
    "video_info = load_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: \n",
      "Text: Confidence went ðŸ“ˆ\n",
      "Hashtags: []\n"
     ]
    }
   ],
   "source": [
    "# Create an object to store the combined information\n",
    "video_data = {}\n",
    "\n",
    "# Combine the information from transcriptions and the JSON file\n",
    "for video_id, info in video_info.items():\n",
    "    if video_id in transcriptions:\n",
    "        general = info.get('general', '')  # Get the general information of the video or an empty string if not present\n",
    "        text = general.get('text', '')  # Get the text of the video or an empty string if not present\n",
    "        hashtags = general.get('hashtags', [])  # Get the hashtags of the video or an empty list if not present\n",
    "        video_data[video_id] = {'transcription': transcriptions[video_id], 'text': text, 'hashtags': hashtags}\n",
    "\n",
    "# Example of accessing the combined information for the first video\n",
    "first_video = list(video_data.keys())[0]\n",
    "print(\"Transcription:\", video_data[first_video]['transcription'])\n",
    "print(\"Text:\", video_data[first_video]['text'])\n",
    "print(\"Hashtags:\", video_data[first_video]['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-empty transcriptions: 261\n",
      "Number of video_data elements with non-empty transcriptions: 261\n",
      "Number of video_data elements with non-empty text: 962\n",
      "Number of video_data elements with non-empty hashtags: 853\n"
     ]
    }
   ],
   "source": [
    "# Counting the elements that are not empty strings in transcriptions\n",
    "transcription_count = sum(1 for trans in transcriptions.values() if trans != '')\n",
    "print(\"Number of non-empty transcriptions:\", transcription_count)\n",
    "\n",
    "# Counting the number of elements in video_data where the transcription is not ''\n",
    "video_data_count_non_empty = sum(1 for video_id, data in video_data.items() if data['transcription'] != '')\n",
    "print(\"Number of video_data elements with non-empty transcriptions:\", video_data_count_non_empty)\n",
    "\n",
    "# Counting the elements that are not empty strings in the video text\n",
    "texts_count = sum(1 for video_id, data in video_data.items() if data['text'] != '')\n",
    "print(\"Number of video_data elements with non-empty text:\", texts_count)\n",
    "\n",
    "# Counting the elements that are not empty lists in hashtags\n",
    "hashtags_count = sum(1 for video_id, data in video_data.items() if data['hashtags'] != [])\n",
    "print(\"Number of video_data elements with non-empty hashtags:\", hashtags_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a transcription for 1 out of 4 videos aprox. Nevertheless, we have text and hashtags for almost all of the videos, so a text strudy could be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning of transcriptions and text\n",
    "for video_id, data in video_data.items():\n",
    "    if 'transcription' in data:\n",
    "        video_data[video_id]['clean_transcription'] = clean_text(data['transcription'])\n",
    "    if 'text' in data:\n",
    "        video_data[video_id]['clean_text'] = clean_text(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcription': \"is there the top 10 strongest one piece characters by the end of the show at number 10 we have Trafalgar law he can make anyone Immortal at the expense of his life number 9 we have useless kid he challenged kaido even though the odds are against him just like Luffy number 8 we have Sabo even though he's not at his Peak he can challenge fujitora and number 7 we have GARP the only man known to go to Toe with the former pirate king Goldie Rodger number 6 we have a kind of the author himself Oda said that if a kind of was the main character he'd find the one piece in a single year number five we have Shanks at the marineford where he just came in and said war is over and it really ended what else do you need to know about Shanks make sure like for part two\",\n",
       " 'text': '#Top10 Strongest #onepiece Characters by the end of the show. #anime #strongestcharacters #animeboy #luffy #zoro #animeedit #animeedits #animestiktok',\n",
       " 'hashtags': ['top10',\n",
       "  'onepiece',\n",
       "  'anime',\n",
       "  'strongestcharacters',\n",
       "  'animeboy',\n",
       "  'luffy',\n",
       "  'zoro',\n",
       "  'animeedit',\n",
       "  'animeedits',\n",
       "  'animestiktok'],\n",
       " 'clean_transcription': 'top 10 strongest one piece characters end show number 10 trafalgar law make anyone immortal expense life number 9 useless kid challenged kaido even though odds like luffy number 8 sabo even though peak challenge fujitora number 7 garp man known go toe former pirate king goldie rodger number 6 kind author oda said kind main character find one piece single year number five shanks marineford came said war really ended else need know shanks make sure like part two',\n",
       " 'clean_text': 'strongest characters end show'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data[list(video_data.keys())[34]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it has occured a tokenization, lemmatization and an elimination of hashtags, punctuation signs, special characters and stopwords for both text and transcription, bearing in mind the language of the possible text and transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be the frequency distribution and the word cloud of the clean transcripts and texts, as well as the hashtags (probably the most useful one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect clean transcripts, clean texts, and hashtags from video_info\n",
    "clean_transcripts = []\n",
    "clean_texts = []\n",
    "hashtags = []\n",
    "for info in video_data.values():\n",
    "    if 'clean_transcription' in info:\n",
    "        clean_transcripts.extend(word_tokenize(info['clean_transcription']))\n",
    "    if 'clean_text' in info:\n",
    "        clean_texts.extend(word_tokenize(info['clean_text']))\n",
    "    if 'hashtags' in info:\n",
    "        hashtags.extend(info['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Transcripts:\n",
      "<FreqDist with 2013 samples and 5252 outcomes>\n",
      "Most common words: [('like', 84), ('know', 67), ('get', 46), ('going', 46), ('yeah', 46), ('want', 41), ('people', 40), ('one', 40), ('really', 35), ('way', 32)]\n"
     ]
    }
   ],
   "source": [
    "# Plot frequency distribution and word cloud for clean transcripts\n",
    "print(\"Analysis of Transcripts:\")\n",
    "plot_freq_dist(clean_transcripts)\n",
    "analyze_text(' '.join(clean_transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Captions:\n",
      "<FreqDist with 1955 samples and 2838 outcomes>\n",
      "Most common words: [('reply', 59), ('one', 17), ('video', 15), ('day', 13), ('1', 12), ('know', 12), ('love', 11), ('jij', 10), ('antwoorden', 10), ('get', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Plot frequency distribution and word cloud for clean texts\n",
    "print(\"Analysis of Captions:\")\n",
    "plot_freq_dist(clean_texts)\n",
    "analyze_text(' '.join(clean_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Hashtags:\n",
      "<FreqDist with 2220 samples and 5330 outcomes>\n",
      "Most common words: [('fyp', 417), ('foryou', 272), ('foryoupage', 174), ('fy', 116), ('fitness', 87), ('workout', 73), ('voorjou', 72), ('viral', 65), ('animeedit', 63), ('anime', 50)]\n"
     ]
    }
   ],
   "source": [
    "# Plot frequency distribution and word cloud for hashtags\n",
    "print(\"Analysis of Hashtags:\")\n",
    "plot_freq_dist(hashtags)\n",
    "analyze_text(' '.join(hashtags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
